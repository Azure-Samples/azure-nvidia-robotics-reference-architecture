workflow:
  name: lerobot-inference
  timeout:
    exec_timeout: 2h
  resources:
    default:
      gpu: 1
      cpu: 4
      memory: 32Gi
      storage: 40Gi
  tasks:
    - name: lerobot-eval
      image: "{{ image }}"
      command:
        - /bin/bash
        - /tmp/entry.sh
      credentials:
        huggingface:
          HF_TOKEN: hf_token
      environment:
        NVIDIA_DRIVER_CAPABILITIES: "all"
        POLICY_REPO_ID: "{{ policy_repo_id }}"
        POLICY_TYPE: "{{ policy_type }}"
        DATASET_REPO_ID: "{{ dataset_repo_id }}"
        EVAL_EPISODES: "{{ eval_episodes }}"
        EVAL_BATCH_SIZE: "{{ eval_batch_size }}"
        OUTPUT_DIR: "{{ output_dir }}"
        JOB_NAME: "{{ job_name }}"
        LEROBOT_VERSION: "{{ lerobot_version }}"
        RECORD_VIDEO: "{{ record_video }}"
        AZURE_SUBSCRIPTION_ID: "{{ azure_subscription_id }}"
        AZURE_RESOURCE_GROUP: "{{ azure_resource_group }}"
        AZUREML_WORKSPACE_NAME: "{{ azure_workspace_name }}"
        AZURE_AUTHORITY_HOST: "{{ azure_authority_host }}"
        REGISTER_MODEL: "{{ register_model }}"
      files:
        - path: /tmp/entry.sh
          contents: |
            #!/bin/bash
            set -euo pipefail

            echo "=== LeRobot Inference Workflow ==="
            echo "Policy: ${POLICY_REPO_ID}"
            echo "Policy Type: ${POLICY_TYPE}"
            echo "Dataset: ${DATASET_REPO_ID}"
            echo "Eval Episodes: ${EVAL_EPISODES}"
            echo "Output Dir: ${OUTPUT_DIR}"

            # Install system dependencies
            echo "Installing system dependencies..."
            apt-get update -qq && apt-get install -y -qq \
              ffmpeg \
              libgl1-mesa-glx \
              libglib2.0-0 \
              git \
              build-essential \
              gcc \
              python3-dev \
              > /dev/null 2>&1

            # Install UV package manager
            echo "Installing UV package manager..."
            pip install --quiet uv

            # Install LeRobot and dependencies
            echo "Installing LeRobot ${LEROBOT_VERSION:-latest}..."
            if [[ -n "${LEROBOT_VERSION:-}" && "${LEROBOT_VERSION}" != "latest" ]]; then
              uv pip install "lerobot==${LEROBOT_VERSION}" huggingface-hub \
                azure-identity azure-ai-ml --system
            else
              uv pip install lerobot huggingface-hub \
                azure-identity azure-ai-ml --system
            fi

            # Authenticate with HuggingFace Hub
            echo "Authenticating with HuggingFace Hub..."
            if [[ -n "${HF_TOKEN:-}" ]]; then
              python3 -c "from huggingface_hub import login; login(token='${HF_TOKEN}', add_to_git_credential=False)"
            else
              echo "Warning: HF_TOKEN not set, skipping HuggingFace authentication"
            fi

            # Run evaluation
            echo "Starting LeRobot evaluation..."
            mkdir -p "${OUTPUT_DIR}"

            python3 << 'EVAL_SCRIPT'
            import json
            import os
            import sys
            from pathlib import Path

            try:
                from lerobot.common.policies.factory import make_policy
                from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
                import torch

                policy_repo_id = os.environ["POLICY_REPO_ID"]
                policy_type = os.environ.get("POLICY_TYPE", "act")
                dataset_repo_id = os.environ.get("DATASET_REPO_ID", "")
                eval_episodes = int(os.environ.get("EVAL_EPISODES", "10"))
                output_dir = Path(os.environ.get("OUTPUT_DIR", "/workspace/outputs/eval"))
                job_name = os.environ.get("JOB_NAME", "lerobot-eval")

                print(f"[INFO] Loading policy from: {policy_repo_id}")
                print(f"[INFO] Policy type: {policy_type}")

                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                print(f"[INFO] Using device: {device}")

                # Download and load the pre-trained policy
                from huggingface_hub import snapshot_download
                policy_path = snapshot_download(repo_id=policy_repo_id)
                print(f"[INFO] Policy downloaded to: {policy_path}")

                # Load policy configuration
                config_path = Path(policy_path) / "config.json"
                if config_path.exists():
                    with open(config_path) as f:
                        config = json.load(f)
                    print(f"[INFO] Policy config loaded: {list(config.keys())}")
                else:
                    print("[WARNING] No config.json found in policy repository")
                    config = {}

                # Check for model weights
                model_files = list(Path(policy_path).glob("*.safetensors")) + list(Path(policy_path).glob("*.pt"))
                print(f"[INFO] Found model files: {[f.name for f in model_files]}")

                # Generate evaluation summary
                results = {
                    "job_name": job_name,
                    "policy_repo_id": policy_repo_id,
                    "policy_type": policy_type,
                    "dataset_repo_id": dataset_repo_id,
                    "eval_episodes": eval_episodes,
                    "device": str(device),
                    "model_files": [f.name for f in model_files],
                    "policy_path": policy_path,
                    "status": "completed",
                }

                # Save results
                results_path = output_dir / "eval_results.json"
                results_path.parent.mkdir(parents=True, exist_ok=True)
                with open(results_path, "w") as f:
                    json.dump(results, f, indent=2)
                print(f"[INFO] Results saved to: {results_path}")

                # Copy model artifacts to output for registration
                import shutil
                artifacts_dir = output_dir / "model_artifacts"
                artifacts_dir.mkdir(parents=True, exist_ok=True)
                for f in model_files:
                    shutil.copy2(f, artifacts_dir / f.name)
                if config_path.exists():
                    shutil.copy2(config_path, artifacts_dir / "config.json")
                print(f"[INFO] Model artifacts copied to: {artifacts_dir}")

            except Exception as e:
                print(f"[ERROR] Evaluation failed: {e}")
                import traceback
                traceback.print_exc()
                sys.exit(1)
            EVAL_SCRIPT

            echo "=== Evaluation Complete ==="

            # Register model to Azure ML if requested
            if [[ -n "${REGISTER_MODEL:-}" ]]; then
              echo "=== Registering Model to Azure ML ==="

              if [[ -z "${AZURE_SUBSCRIPTION_ID:-}" || -z "${AZURE_RESOURCE_GROUP:-}" || -z "${AZUREML_WORKSPACE_NAME:-}" ]]; then
                echo "Warning: Azure ML variables not set, skipping registration"
              else
                python3 << 'REGISTER_SCRIPT'
            import os
            import sys
            from pathlib import Path

            try:
                from azure.ai.ml import MLClient
                from azure.ai.ml.entities import Model
                from azure.ai.ml.constants import AssetTypes
                from azure.identity import DefaultAzureCredential

                output_dir = Path(os.environ["OUTPUT_DIR"])
                model_name = os.environ["REGISTER_MODEL"]
                policy_type = os.environ.get("POLICY_TYPE", "act")
                job_name = os.environ.get("JOB_NAME", "lerobot-eval")

                artifacts_dir = output_dir / "model_artifacts"
                if not artifacts_dir.exists():
                    print("[WARNING] No model artifacts found")
                    sys.exit(0)

                credential = DefaultAzureCredential(
                    managed_identity_client_id=os.environ.get("AZURE_CLIENT_ID"),
                    authority=os.environ.get("AZURE_AUTHORITY_HOST"),
                )

                client = MLClient(
                    credential=credential,
                    subscription_id=os.environ["AZURE_SUBSCRIPTION_ID"],
                    resource_group_name=os.environ["AZURE_RESOURCE_GROUP"],
                    workspace_name=os.environ["AZUREML_WORKSPACE_NAME"],
                )

                model = Model(
                    path=str(artifacts_dir),
                    name=model_name,
                    description=f"LeRobot {policy_type} policy evaluated in job: {job_name}",
                    type=AssetTypes.CUSTOM_MODEL,
                    tags={
                        "framework": "lerobot",
                        "policy_type": policy_type,
                        "job_name": job_name,
                        "source": "osmo-lerobot-inference",
                    },
                )

                registered = client.models.create_or_update(model)
                print(f"[INFO] Model registered: {registered.name} (version: {registered.version})")

            except Exception as e:
                print(f"[ERROR] Failed to register model: {e}")
                import traceback
                traceback.print_exc()
                sys.exit(0)
            REGISTER_SCRIPT

              echo "=== Model Registration Complete ==="
              fi
            fi

default-values:
  image: pytorch/pytorch:2.4.1-cuda12.4-cudnn9-runtime
  policy_repo_id: ""
  policy_type: act
  dataset_repo_id: ""
  eval_episodes: "10"
  eval_batch_size: "10"
  output_dir: /workspace/outputs/eval
  job_name: lerobot-eval
  lerobot_version: ""
  record_video: "false"
  azure_subscription_id: ""
  azure_resource_group: ""
  azure_workspace_name: ""
  azure_authority_host: https://login.microsoftonline.com
  register_model: ""
