workflow:
  name: lerobot-inference
  timeout:
    exec_timeout: 2h
  resources:
    default:
      gpu: 1
      cpu: 4
      memory: 32Gi
      storage: 40Gi
  tasks:
    - name: lerobot-eval
      image: "{{ image }}"
      command:
        - /bin/bash
        - /tmp/entry.sh
      credentials:
        huggingface:
          HF_TOKEN: hf_token
      environment:
        NVIDIA_DRIVER_CAPABILITIES: "all"
        POLICY_REPO_ID: "{{ policy_repo_id }}"
        POLICY_TYPE: "{{ policy_type }}"
        DATASET_REPO_ID: "{{ dataset_repo_id }}"
        EVAL_EPISODES: "{{ eval_episodes }}"
        EVAL_BATCH_SIZE: "{{ eval_batch_size }}"
        OUTPUT_DIR: "{{ output_dir }}"
        JOB_NAME: "{{ job_name }}"
        LEROBOT_VERSION: "{{ lerobot_version }}"
        RECORD_VIDEO: "{{ record_video }}"
        MLFLOW_ENABLE: "{{ mlflow_enable }}"
        EXPERIMENT_NAME: "{{ experiment_name }}"
        AZURE_SUBSCRIPTION_ID: "{{ azure_subscription_id }}"
        AZURE_RESOURCE_GROUP: "{{ azure_resource_group }}"
        AZUREML_WORKSPACE_NAME: "{{ azure_workspace_name }}"
        AZURE_AUTHORITY_HOST: "{{ azure_authority_host }}"
        MLFLOW_TRACKING_TOKEN_REFRESH_RETRIES: "{{ mlflow_token_refresh_retries }}"
        MLFLOW_HTTP_REQUEST_TIMEOUT: "{{ mlflow_http_request_timeout }}"
        REGISTER_MODEL: "{{ register_model }}"
      files:
        - path: /tmp/entry.sh
          contents: |
            #!/bin/bash
            set -euo pipefail

            echo "=== LeRobot Inference Workflow ==="
            echo "Policy: ${POLICY_REPO_ID}"
            echo "Policy Type: ${POLICY_TYPE}"
            echo "Dataset: ${DATASET_REPO_ID}"
            echo "Eval Episodes: ${EVAL_EPISODES}"
            echo "Output Dir: ${OUTPUT_DIR}"

            # Install system dependencies
            echo "Installing system dependencies..."
            apt-get update -qq && apt-get install -y -qq \
              ffmpeg \
              libgl1-mesa-glx \
              libglib2.0-0 \
              git \
              build-essential \
              gcc \
              python3-dev \
              > /dev/null 2>&1

            # Install UV package manager
            echo "Installing UV package manager..."
            pip install --quiet uv

            # Install LeRobot and dependencies
            echo "Installing LeRobot ${LEROBOT_VERSION:-latest}..."
            if [[ -n "${LEROBOT_VERSION:-}" && "${LEROBOT_VERSION}" != "latest" ]]; then
              uv pip install "lerobot==${LEROBOT_VERSION}" huggingface-hub \
                azure-identity azure-ai-ml --system
            else
              uv pip install lerobot huggingface-hub \
                azure-identity azure-ai-ml --system
            fi

            # Install MLflow and matplotlib when MLflow tracking is enabled
            if [[ "${MLFLOW_ENABLE:-false}" == "true" ]]; then
              echo "Installing MLflow and plotting dependencies..."
              uv pip install azureml-mlflow "mlflow>=2.8.0,<3.0.0" matplotlib --system
            fi

            # Authenticate with HuggingFace Hub
            echo "Authenticating with HuggingFace Hub..."
            if [[ -n "${HF_TOKEN:-}" ]]; then
              python3 -c "from huggingface_hub import login; login(token='${HF_TOKEN}', add_to_git_credential=False)"
            else
              echo "Warning: HF_TOKEN not set, skipping HuggingFace authentication"
            fi

            # Bootstrap MLflow tracking
            if [[ "${MLFLOW_ENABLE:-false}" == "true" ]]; then
              echo "Configuring Azure ML MLflow tracking..."

              if [[ -z "${AZURE_SUBSCRIPTION_ID:-}" || -z "${AZURE_RESOURCE_GROUP:-}" || -z "${AZUREML_WORKSPACE_NAME:-}" ]]; then
                echo "Error: MLflow requires AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, and AZUREML_WORKSPACE_NAME"
                exit 1
              fi

              python3 << 'BOOTSTRAP_SCRIPT'
            import os
            import sys

            try:
                import mlflow
                from azure.ai.ml import MLClient
                from azure.identity import DefaultAzureCredential

                credential = DefaultAzureCredential(
                    managed_identity_client_id=os.environ.get("AZURE_CLIENT_ID"),
                    authority=os.environ.get("AZURE_AUTHORITY_HOST"),
                )

                client = MLClient(
                    credential=credential,
                    subscription_id=os.environ["AZURE_SUBSCRIPTION_ID"],
                    resource_group_name=os.environ["AZURE_RESOURCE_GROUP"],
                    workspace_name=os.environ["AZUREML_WORKSPACE_NAME"],
                )

                workspace = client.workspaces.get(os.environ["AZUREML_WORKSPACE_NAME"])
                tracking_uri = workspace.mlflow_tracking_uri

                if not tracking_uri:
                    print("[ERROR] Azure ML workspace does not expose MLflow tracking URI")
                    sys.exit(1)

                mlflow.set_tracking_uri(tracking_uri)
                experiment_name = os.environ.get("EXPERIMENT_NAME") or f"lerobot-{os.environ.get('POLICY_TYPE', 'act')}-inference"
                mlflow.set_experiment(experiment_name)

                with open("/tmp/mlflow_config.env", "w") as f:
                    f.write(f"MLFLOW_TRACKING_URI={tracking_uri}\n")
                    f.write(f"MLFLOW_EXPERIMENT_NAME={experiment_name}\n")

                print(f"[INFO] MLflow configured: {experiment_name}")

            except Exception as e:
                print(f"[ERROR] Failed to configure Azure ML: {e}")
                sys.exit(1)
            BOOTSTRAP_SCRIPT

              if [[ -f /tmp/mlflow_config.env ]]; then
                export $(cat /tmp/mlflow_config.env | xargs)
              fi
            fi

            # Run evaluation
            echo "Starting LeRobot evaluation..."
            mkdir -p "${OUTPUT_DIR}"

            python3 << 'EVAL_SCRIPT'
            import json
            import os
            import sys
            import time
            from pathlib import Path

            import numpy as np
            import torch

            JOINT_NAMES = [
                "shoulder_pan", "shoulder_lift", "elbow",
                "wrist_1", "wrist_2", "wrist_3",
            ]

            # ----------------------------------------------------------------
            # Plotting helpers
            # ----------------------------------------------------------------

            def plot_action_deltas(predicted, ground_truth, episode, fps):
                """Per-joint action deltas overlay."""
                import matplotlib
                matplotlib.use("Agg")
                import matplotlib.pyplot as plt

                n_steps, n_joints = predicted.shape
                t = np.arange(n_steps) / fps
                fig, axes = plt.subplots(n_joints, 1, figsize=(14, 2.5 * n_joints), sharex=True)
                fig.suptitle(f"Episode {episode} — Action Deltas: Predicted vs Ground Truth", fontsize=14, fontweight="bold")
                for j, ax in enumerate(axes):
                    ax.plot(t, ground_truth[:, j], color="#2196F3", alpha=0.8, linewidth=1.2, label="Ground Truth")
                    ax.plot(t, predicted[:, j], color="#FF5722", alpha=0.8, linewidth=1.2, label="Predicted")
                    ax.fill_between(t, ground_truth[:, j], predicted[:, j], alpha=0.15, color="#9C27B0")
                    ax.set_ylabel(f"{JOINT_NAMES[j]}\n(rad)", fontsize=9)
                    ax.grid(True, alpha=0.3)
                    if j == 0:
                        ax.legend(loc="upper right", fontsize=8)
                axes[-1].set_xlabel("Time (s)", fontsize=10)
                fig.tight_layout()
                return fig


            def plot_cumulative_positions(predicted, ground_truth, episode, fps):
                """Reconstructed absolute joint positions from cumulative deltas."""
                import matplotlib
                matplotlib.use("Agg")
                import matplotlib.pyplot as plt

                pred_pos = np.cumsum(predicted, axis=0)
                gt_pos = np.cumsum(ground_truth, axis=0)
                n_steps, n_joints = predicted.shape
                t = np.arange(n_steps) / fps
                fig, axes = plt.subplots(n_joints, 1, figsize=(14, 2.5 * n_joints), sharex=True)
                fig.suptitle(f"Episode {episode} — Reconstructed Joint Positions", fontsize=14, fontweight="bold")
                for j, ax in enumerate(axes):
                    ax.plot(t, gt_pos[:, j], color="#2196F3", alpha=0.8, linewidth=1.2, label="Ground Truth")
                    ax.plot(t, pred_pos[:, j], color="#FF5722", alpha=0.8, linewidth=1.2, label="Predicted")
                    ax.fill_between(t, gt_pos[:, j], pred_pos[:, j], alpha=0.15, color="#9C27B0")
                    ax.set_ylabel(f"{JOINT_NAMES[j]}\n(rad)", fontsize=9)
                    ax.grid(True, alpha=0.3)
                    if j == 0:
                        ax.legend(loc="upper right", fontsize=8)
                axes[-1].set_xlabel("Time (s)", fontsize=10)
                fig.tight_layout()
                return fig


            def plot_error_heatmap(predicted, ground_truth, episode, fps):
                """Absolute error heatmap across joints and time."""
                import matplotlib
                matplotlib.use("Agg")
                import matplotlib.pyplot as plt

                error = np.abs(predicted - ground_truth)
                n_steps = error.shape[0]
                t = np.arange(n_steps) / fps
                fig, ax = plt.subplots(figsize=(14, 3))
                im = ax.imshow(
                    error.T, aspect="auto", cmap="hot", interpolation="nearest",
                    extent=[t[0], t[-1], len(JOINT_NAMES) - 0.5, -0.5],
                )
                ax.set_yticks(range(len(JOINT_NAMES)))
                ax.set_yticklabels(JOINT_NAMES, fontsize=9)
                ax.set_xlabel("Time (s)", fontsize=10)
                ax.set_title(f"Episode {episode} — Absolute Error Heatmap", fontsize=12, fontweight="bold")
                fig.colorbar(im, ax=ax, label="Error (rad)")
                fig.tight_layout()
                return fig


            def plot_summary_panel(predicted, ground_truth, inference_times, episode, fps):
                """2x2 summary: all joints overlay, error boxplots, latency, per-joint MAE."""
                import matplotlib
                matplotlib.use("Agg")
                import matplotlib.pyplot as plt

                error = np.abs(predicted - ground_truth)
                n_steps, n_joints = predicted.shape
                t = np.arange(n_steps) / fps
                colors = plt.cm.tab10(np.linspace(0, 1, n_joints))

                fig, axes = plt.subplots(2, 2, figsize=(14, 8))
                fig.suptitle(f"Episode {episode} — Inference Summary", fontsize=14, fontweight="bold")

                ax = axes[0, 0]
                for j in range(n_joints):
                    ax.plot(t, ground_truth[:, j], color=colors[j], alpha=0.6, linewidth=1.0)
                    ax.plot(t, predicted[:, j], color=colors[j], alpha=0.6, linewidth=1.0, linestyle="--")
                ax.set_xlabel("Time (s)", fontsize=9)
                ax.set_ylabel("Action delta (rad)", fontsize=9)
                ax.set_title("All Joints (solid=GT, dashed=pred)", fontsize=10)
                ax.grid(True, alpha=0.3)

                ax = axes[0, 1]
                ax.boxplot([error[:, j] for j in range(n_joints)], tick_labels=JOINT_NAMES, patch_artist=True)
                ax.set_ylabel("Absolute Error (rad)", fontsize=9)
                ax.set_title("Error Distribution per Joint", fontsize=10)
                ax.tick_params(axis="x", rotation=30, labelsize=8)
                ax.grid(True, alpha=0.3, axis="y")

                ax = axes[1, 0]
                inf_ms = inference_times * 1000
                ax.plot(inf_ms, color="#4CAF50", alpha=0.7, linewidth=0.8)
                ax.axhline(y=1000 / fps, color="#F44336", linestyle="--", alpha=0.7, label=f"Realtime ({1000/fps:.1f}ms)")
                ax.set_xlabel("Step", fontsize=9)
                ax.set_ylabel("Inference time (ms)", fontsize=9)
                ax.set_title("Inference Latency", fontsize=10)
                ax.legend(fontsize=8)
                ax.grid(True, alpha=0.3)
                ax.set_ylim(0, min(np.percentile(inf_ms, 99) * 2, inf_ms.max() * 1.1))

                ax = axes[1, 1]
                per_joint_mae = np.mean(error, axis=0)
                bars = ax.bar(JOINT_NAMES, per_joint_mae, color=colors[:n_joints], alpha=0.7)
                ax.set_ylabel("MAE (rad)", fontsize=9)
                ax.set_title("Per-Joint Mean Absolute Error", fontsize=10)
                ax.tick_params(axis="x", rotation=30, labelsize=8)
                ax.grid(True, alpha=0.3, axis="y")
                for bar, val in zip(bars, per_joint_mae):
                    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f"{val:.4f}", ha="center", va="bottom", fontsize=7)

                fig.tight_layout()
                return fig

            # ----------------------------------------------------------------
            # Inference helpers
            # ----------------------------------------------------------------

            def load_video_frame(dataset_dir, episode, frame):
                import av
                video_path = os.path.join(
                    dataset_dir, "videos", "observation.images.color",
                    f"chunk-{episode:03d}", f"file-{episode:03d}.mp4",
                )
                container = av.open(video_path)
                stream = container.streams.video[0]
                for i, av_frame in enumerate(container.decode(stream)):
                    if i == frame:
                        img = av_frame.to_ndarray(format="rgb24")
                        container.close()
                        return img
                container.close()
                raise IndexError(f"Frame {frame} not found in {video_path}")


            def load_episode_data(dataset_dir, episode):
                import pyarrow.parquet as pq
                data_path = os.path.join(dataset_dir, "data", f"chunk-{episode:03d}", f"file-{episode:03d}.parquet")
                table = pq.read_table(data_path)
                return {col: table[col].to_pylist() for col in table.column_names}


            def build_observation(state, image):
                return {
                    "observation.state": torch.from_numpy(state).float(),
                    "observation.images.color": (torch.from_numpy(image).float().permute(2, 0, 1) / 255.0),
                }

            # ----------------------------------------------------------------
            # Main evaluation
            # ----------------------------------------------------------------

            try:
                from lerobot.policies.act.modeling_act import ACTPolicy
                from lerobot.processor.pipeline import PolicyProcessorPipeline

                policy_repo_id = os.environ["POLICY_REPO_ID"]
                policy_type = os.environ.get("POLICY_TYPE", "act")
                dataset_repo_id = os.environ.get("DATASET_REPO_ID", "")
                eval_episodes = int(os.environ.get("EVAL_EPISODES", "10"))
                output_dir = Path(os.environ.get("OUTPUT_DIR", "/workspace/outputs/eval"))
                job_name = os.environ.get("JOB_NAME", "lerobot-eval")
                mlflow_enable = os.environ.get("MLFLOW_ENABLE", "false") == "true"

                output_dir.mkdir(parents=True, exist_ok=True)

                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                print(f"[INFO] Using device: {device}")

                # Download dataset
                if dataset_repo_id:
                    from huggingface_hub import snapshot_download
                    dataset_dir = snapshot_download(repo_id=dataset_repo_id, repo_type="dataset")
                    print(f"[INFO] Dataset downloaded to: {dataset_dir}")
                else:
                    print("[ERROR] --dataset-repo-id is required for evaluation")
                    sys.exit(1)

                # Load dataset info
                with open(os.path.join(dataset_dir, "meta", "info.json")) as f:
                    info = json.load(f)
                fps = info["fps"]

                # Load policy
                print(f"[INFO] Loading policy from: {policy_repo_id}")
                policy = ACTPolicy.from_pretrained(policy_repo_id)
                policy.to(device)

                device_override = {"device_processor": {"device": str(device)}}
                preprocessor = PolicyProcessorPipeline.from_pretrained(
                    policy_repo_id, "policy_preprocessor.json", overrides=device_override,
                )
                postprocessor = PolicyProcessorPipeline.from_pretrained(
                    policy_repo_id, "policy_postprocessor.json", overrides=device_override,
                )

                # Determine episode range
                episodes_meta_path = os.path.join(dataset_dir, "meta", "episodes.jsonl")
                if os.path.exists(episodes_meta_path):
                    with open(episodes_meta_path) as f:
                        total_episodes = sum(1 for _ in f)
                else:
                    total_episodes = eval_episodes
                num_episodes = min(eval_episodes, total_episodes)

                # Start MLflow run
                mlflow_run = None
                if mlflow_enable:
                    import mlflow
                    mlflow_run = mlflow.start_run(run_name=job_name)
                    mlflow.log_params({
                        "policy_repo_id": policy_repo_id,
                        "policy_type": policy_type,
                        "dataset_repo_id": dataset_repo_id,
                        "eval_episodes": num_episodes,
                        "device": str(device),
                        "fps": fps,
                    })

                all_episode_metrics = []

                for ep in range(num_episodes):
                    print(f"\n{'='*60}")
                    print(f"Episode {ep}")
                    print(f"{'='*60}")

                    data = load_episode_data(dataset_dir, ep)
                    n_frames = len(data["timestamp"])

                    policy.reset()
                    actions_predicted = []
                    actions_ground_truth = []
                    inference_times = []

                    for step in range(n_frames - 1):
                        state = np.array(data["observation.state"][step], dtype=np.float32)
                        gt_action = np.array(data["action"][step], dtype=np.float32)
                        image = load_video_frame(dataset_dir, ep, step)

                        obs = build_observation(state, image)
                        obs = preprocessor(obs)

                        t_start = time.time()
                        with torch.inference_mode():
                            action = policy.select_action(obs)
                        t_inf = time.time() - t_start
                        inference_times.append(t_inf)

                        action = postprocessor({"action": action})
                        action_np = action["action"].squeeze(0).cpu().numpy()
                        actions_predicted.append(action_np)
                        actions_ground_truth.append(gt_action)

                    pred = np.array(actions_predicted)
                    gt = np.array(actions_ground_truth)
                    inf_times = np.array(inference_times)

                    mse = float(np.mean((pred - gt) ** 2))
                    mae = float(np.mean(np.abs(pred - gt)))
                    per_joint_mae = np.mean(np.abs(pred - gt), axis=0)
                    avg_inf_ms = float(np.mean(inf_times) * 1000)
                    throughput = float(1.0 / np.mean(inf_times))

                    print(f"  Steps: {len(pred)}, MSE: {mse:.6f}, MAE: {mae:.6f}")
                    print(f"  Avg inference: {avg_inf_ms:.1f}ms, Throughput: {throughput:.1f} Hz")

                    ep_metrics = {
                        "episode": ep,
                        "steps": len(pred),
                        "mse": mse,
                        "mae": mae,
                        "avg_inference_ms": avg_inf_ms,
                        "throughput_hz": throughput,
                        "per_joint_mae": per_joint_mae.tolist(),
                    }
                    all_episode_metrics.append(ep_metrics)

                    # Save .npz
                    npz_path = output_dir / f"ep{ep:03d}_predictions.npz"
                    np.savez(npz_path, predicted=pred, ground_truth=gt, inference_times=inf_times)

                    # Generate plots and log to MLflow
                    if mlflow_enable:
                        import mlflow
                        import matplotlib
                        matplotlib.use("Agg")
                        import matplotlib.pyplot as plt

                        mlflow.log_metrics({
                            f"ep{ep}_mse": mse,
                            f"ep{ep}_mae": mae,
                            f"ep{ep}_avg_inference_ms": avg_inf_ms,
                            f"ep{ep}_throughput_hz": throughput,
                        })
                        for j, name in enumerate(JOINT_NAMES):
                            mlflow.log_metric(f"ep{ep}_mae_{name}", float(per_joint_mae[j]))

                        fig = plot_action_deltas(pred, gt, ep, fps)
                        mlflow.log_figure(fig, f"plots/episode_{ep:03d}/action_deltas.png")
                        plt.close(fig)

                        fig = plot_cumulative_positions(pred, gt, ep, fps)
                        mlflow.log_figure(fig, f"plots/episode_{ep:03d}/cumulative_positions.png")
                        plt.close(fig)

                        fig = plot_error_heatmap(pred, gt, ep, fps)
                        mlflow.log_figure(fig, f"plots/episode_{ep:03d}/error_heatmap.png")
                        plt.close(fig)

                        fig = plot_summary_panel(pred, gt, inf_times, ep, fps)
                        mlflow.log_figure(fig, f"plots/episode_{ep:03d}/summary_panel.png")
                        plt.close(fig)

                        mlflow.log_artifact(str(npz_path), "predictions")

                        print(f"  Logged 4 plots + metrics to MLflow for episode {ep}")

                # Aggregate metrics
                agg_mse = float(np.mean([m["mse"] for m in all_episode_metrics]))
                agg_mae = float(np.mean([m["mae"] for m in all_episode_metrics]))
                agg_inf_ms = float(np.mean([m["avg_inference_ms"] for m in all_episode_metrics]))
                agg_throughput = float(np.mean([m["throughput_hz"] for m in all_episode_metrics]))

                results = {
                    "job_name": job_name,
                    "policy_repo_id": policy_repo_id,
                    "policy_type": policy_type,
                    "dataset_repo_id": dataset_repo_id,
                    "device": str(device),
                    "episodes_evaluated": num_episodes,
                    "aggregate_mse": agg_mse,
                    "aggregate_mae": agg_mae,
                    "aggregate_avg_inference_ms": agg_inf_ms,
                    "aggregate_throughput_hz": agg_throughput,
                    "per_episode": all_episode_metrics,
                    "status": "completed",
                }

                results_path = output_dir / "eval_results.json"
                with open(results_path, "w") as f:
                    json.dump(results, f, indent=2)
                print(f"\n[INFO] Results saved to: {results_path}")

                if mlflow_enable:
                    import mlflow
                    mlflow.log_metrics({
                        "aggregate_mse": agg_mse,
                        "aggregate_mae": agg_mae,
                        "aggregate_avg_inference_ms": agg_inf_ms,
                        "aggregate_throughput_hz": agg_throughput,
                    })
                    mlflow.log_artifact(str(results_path))
                    mlflow.end_run()
                    print("[INFO] MLflow run completed with plots and metrics")

            except Exception as e:
                print(f"[ERROR] Evaluation failed: {e}")
                import traceback
                traceback.print_exc()
                if mlflow_run:
                    import mlflow
                    mlflow.end_run(status="FAILED")
                sys.exit(1)
            EVAL_SCRIPT

            echo "=== Evaluation Complete ==="

            # Register model to Azure ML if requested
            if [[ -n "${REGISTER_MODEL:-}" ]]; then
              echo "=== Registering Model to Azure ML ==="

              if [[ -z "${AZURE_SUBSCRIPTION_ID:-}" || -z "${AZURE_RESOURCE_GROUP:-}" || -z "${AZUREML_WORKSPACE_NAME:-}" ]]; then
                echo "Warning: Azure ML variables not set, skipping registration"
              else
                python3 << 'REGISTER_SCRIPT'
            import os
            import sys
            from pathlib import Path

            try:
                from azure.ai.ml import MLClient
                from azure.ai.ml.entities import Model
                from azure.ai.ml.constants import AssetTypes
                from azure.identity import DefaultAzureCredential

                output_dir = Path(os.environ["OUTPUT_DIR"])
                model_name = os.environ["REGISTER_MODEL"]
                policy_type = os.environ.get("POLICY_TYPE", "act")
                job_name = os.environ.get("JOB_NAME", "lerobot-eval")

                artifacts_dir = output_dir / "model_artifacts"
                if not artifacts_dir.exists():
                    print("[WARNING] No model artifacts found, skipping registration")
                    sys.exit(0)

                credential = DefaultAzureCredential(
                    managed_identity_client_id=os.environ.get("AZURE_CLIENT_ID"),
                    authority=os.environ.get("AZURE_AUTHORITY_HOST"),
                )

                client = MLClient(
                    credential=credential,
                    subscription_id=os.environ["AZURE_SUBSCRIPTION_ID"],
                    resource_group_name=os.environ["AZURE_RESOURCE_GROUP"],
                    workspace_name=os.environ["AZUREML_WORKSPACE_NAME"],
                )

                model = Model(
                    path=str(artifacts_dir),
                    name=model_name,
                    description=f"LeRobot {policy_type} policy evaluated in job: {job_name}",
                    type=AssetTypes.CUSTOM_MODEL,
                    tags={
                        "framework": "lerobot",
                        "policy_type": policy_type,
                        "job_name": job_name,
                        "source": "osmo-lerobot-inference",
                    },
                )

                registered = client.models.create_or_update(model)
                print(f"[INFO] Model registered: {registered.name} (version: {registered.version})")

            except Exception as e:
                print(f"[ERROR] Failed to register model: {e}")
                import traceback
                traceback.print_exc()
                sys.exit(0)
            REGISTER_SCRIPT

              echo "=== Model Registration Complete ==="
              fi
            fi

default-values:
  image: pytorch/pytorch:2.4.1-cuda12.4-cudnn9-runtime
  policy_repo_id: ""
  policy_type: act
  dataset_repo_id: ""
  eval_episodes: "10"
  eval_batch_size: "10"
  output_dir: /workspace/outputs/eval
  job_name: lerobot-eval
  lerobot_version: ""
  record_video: "false"
  mlflow_enable: "false"
  experiment_name: ""
  azure_subscription_id: ""
  azure_resource_group: ""
  azure_workspace_name: ""
  azure_authority_host: https://login.microsoftonline.com
  mlflow_token_refresh_retries: "5"
  mlflow_http_request_timeout: "600"
  register_model: ""
