workflow:
  name: lerobot-training
  timeout:
    exec_timeout: 12h
  resources:
    default:
      gpu: 1
      cpu: 8
      memory: 48Gi
      storage: 80Gi
  tasks:
    - name: lerobot-train
      image: "{{ image }}"
      command:
        - /bin/bash
        - /tmp/entry.sh
      credentials:
        huggingface:
          HF_TOKEN: hf_token
      environment:
        DATASET_REPO_ID: "{{ dataset_repo_id }}"
        DATASET_ROOT: "{{ dataset_root }}"
        POLICY_TYPE: "{{ policy_type }}"
        OUTPUT_DIR: "{{ output_dir }}"
        JOB_NAME: "{{ job_name }}"
        POLICY_REPO_ID: "{{ policy_repo_id }}"
        TRAINING_STEPS: "{{ training_steps }}"
        BATCH_SIZE: "{{ batch_size }}"
        LEARNING_RATE: "{{ learning_rate }}"
        LR_WARMUP_STEPS: "{{ lr_warmup_steps }}"
        EVAL_FREQ: "{{ eval_freq }}"
        SAVE_FREQ: "{{ save_freq }}"
        VAL_SPLIT: "{{ val_split }}"
        SYSTEM_METRICS: "{{ system_metrics }}"
        LEROBOT_VERSION: "{{ lerobot_version }}"
        EXPERIMENT_NAME: "{{ experiment_name }}"
        REGISTER_CHECKPOINT: "{{ register_checkpoint }}"
        STORAGE_ACCOUNT: "{{ storage_account }}"
        STORAGE_CONTAINER: "{{ storage_container }}"
        BLOB_PREFIX: "{{ blob_prefix }}"
        AZURE_SUBSCRIPTION_ID: "{{ azure_subscription_id }}"
        AZURE_RESOURCE_GROUP: "{{ azure_resource_group }}"
        AZUREML_WORKSPACE_NAME: "{{ azure_workspace_name }}"
        ENCODED_ARCHIVE: "{{ encoded_archive }}"
        PAYLOAD_ROOT: "{{ payload_root }}"
        AZURE_AUTHORITY_HOST: "{{ azure_authority_host }}"
        MLFLOW_TRACKING_TOKEN_REFRESH_RETRIES: "{{ mlflow_token_refresh_retries }}"
        MLFLOW_HTTP_REQUEST_TIMEOUT: "{{ mlflow_http_request_timeout }}"
      files:
        - path: /tmp/entry.sh
          contents: |
            #!/bin/bash
            set -euo pipefail

            echo "=== LeRobot Training Workflow ==="
            echo "Dataset: ${DATASET_REPO_ID}"
            echo "Policy Type: ${POLICY_TYPE}"
            echo "Job Name: ${JOB_NAME}"
            echo "Output Dir: ${OUTPUT_DIR}"
            echo "Logging: Azure ML MLflow"
            echo "Val Split: ${VAL_SPLIT:-0.1}"
            echo "System Metrics: ${SYSTEM_METRICS:-true}"
            if [[ -n "${STORAGE_ACCOUNT:-}" ]]; then
              echo "Data Source: Azure Blob (${STORAGE_ACCOUNT}/${STORAGE_CONTAINER}/${BLOB_PREFIX})"
            else
              echo "Data Source: HuggingFace Hub"
            fi

            # Install system dependencies
            echo "Installing system dependencies..."
            apt-get update -qq && apt-get install -y -qq \
              ffmpeg \
              libgl1-mesa-glx \
              libglib2.0-0 \
              build-essential \
              gcc \
              unzip \
              python3-dev \
              > /dev/null 2>&1

            # Install UV package manager
            echo "Installing UV package manager..."
            pip install --quiet uv

            # Install LeRobot and Azure ML dependencies
            LEROBOT_PKG="lerobot"
            if [[ -n "${LEROBOT_VERSION:-}" && "${LEROBOT_VERSION}" != "latest" ]]; then
              LEROBOT_PKG="lerobot==${LEROBOT_VERSION}"
            fi

            PIP_PACKAGES=(
              "${LEROBOT_PKG}" huggingface-hub
              azure-identity azure-ai-ml azureml-mlflow "mlflow>=2.8.0"
              psutil pynvml
            )

            # Add blob storage dependencies when using Azure data source
            if [[ -n "${STORAGE_ACCOUNT:-}" ]]; then
              PIP_PACKAGES+=(azure-storage-blob pyarrow)
            fi

            echo "Installing LeRobot ${LEROBOT_VERSION:-latest} and dependencies..."
            if command -v uv &>/dev/null; then
              uv pip install "${PIP_PACKAGES[@]}" --system
            else
              pip install --quiet --no-cache-dir "${PIP_PACKAGES[@]}"
            fi

            # Unpack training scripts from base64-encoded payload
            ARCHIVE_PATH="/tmp/lerobot_payload.zip"
            PAYLOAD_ROOT="${PAYLOAD_ROOT:-/workspace/lerobot_payload}"
            mkdir -p "${PAYLOAD_ROOT}"
            if [[ -z "${ENCODED_ARCHIVE:-}" ]]; then
              echo "ERROR: ENCODED_ARCHIVE is not set or empty; training payload is required." >&2
              exit 1
            fi
            printf '%s' "${ENCODED_ARCHIVE}" | base64 -d > "${ARCHIVE_PATH}"
            unzip -oq "${ARCHIVE_PATH}" -d "${PAYLOAD_ROOT}"
            export PYTHONPATH="${PAYLOAD_ROOT}/src:${PYTHONPATH:-}"
            echo "Training scripts unpacked to ${PAYLOAD_ROOT}/src"

            # Build training command arguments
            TRAIN_ARGS=()

            # Download dataset from Azure Blob Storage when configured
            if [[ -n "${STORAGE_ACCOUNT:-}" ]]; then
              echo "Downloading dataset from Azure Blob Storage..."
              python3 -m training.scripts.lerobot.download_dataset

              FULL_DATASET_PATH="${DATASET_ROOT}/${DATASET_REPO_ID}"
              echo "Dataset downloaded to: ${FULL_DATASET_PATH}"
              TRAIN_ARGS+=(
                "--dataset.root=${FULL_DATASET_PATH}"
                "--dataset.use_imagenet_stats=false"
                "--dataset.video_backend=pyav"
                "--tolerance_s=0.04"
              )
            fi

            # Run training via Python orchestrator
            echo "Starting LeRobot training..."
            python3 -m training.scripts.lerobot.train "${TRAIN_ARGS[@]}"

            echo "=== Training Complete ==="
            ls -la "${OUTPUT_DIR}/" 2>/dev/null || true

            # Upload checkpoints to Azure ML model registry
            if [[ -n "${REGISTER_CHECKPOINT:-}" && -n "${AZURE_SUBSCRIPTION_ID:-}" && -n "${AZURE_RESOURCE_GROUP:-}" && -n "${AZUREML_WORKSPACE_NAME:-}" ]]; then
              echo "=== Uploading Checkpoints to Azure ML ==="
              python3 -c "from training.scripts.lerobot.checkpoints import upload_checkpoints_to_azure_ml; upload_checkpoints_to_azure_ml()"
              echo "=== Checkpoint Upload Complete ==="
            fi

default-values:
  image: pytorch/pytorch:2.4.1-cuda12.4-cudnn9-runtime
  encoded_archive: ""
  payload_root: /workspace/lerobot_payload
  dataset_repo_id: ""
  dataset_root: /workspace/data
  policy_type: act
  output_dir: /workspace/outputs/train
  job_name: lerobot-training
  policy_repo_id: ""
  training_steps: "100000"
  batch_size: "32"
  learning_rate: "1e-4"
  lr_warmup_steps: "1000"
  eval_freq: ""
  save_freq: "5000"
  val_split: "0.1"
  system_metrics: "true"
  lerobot_version: ""
  experiment_name: ""
  register_checkpoint: ""
  storage_account: ""
  storage_container: datasets
  blob_prefix: ""
  azure_subscription_id: ""
  azure_resource_group: ""
  azure_workspace_name: ""
  azure_authority_host: https://login.microsoftonline.com
  mlflow_token_refresh_retries: "3"
  mlflow_http_request_timeout: "60"
