# Infrastructure as Code

Terraform configuration for the robotics reference architecture. Deploys Azure resources including AKS with GPU node pools, Azure ML workspace, storage, and OSMO backend services (PostgreSQL, Redis).

## üìã Prerequisites

| Tool         | Version         | Setup or Check                  |
|--------------|-----------------|---------------------------------|
| Azure CLI    | Latest          | `az login`                      |
| Terraform    | 1.5+            | `terraform version`             |
| GPU VM quota | Region-specific | e.g., `Standard_NV36ads_A10_v5` |

### Azure RBAC Permissions

| Role                                    | Scope                                                 |
|-----------------------------------------|-------------------------------------------------------|
| Contributor                             | Subscription (new RG) or Resource Group (existing RG) |
| Role Based Access Control Administrator | Subscription (new RG) or Resource Group (existing RG) |

Terraform creates role assignments for managed identities, requiring `Microsoft.Authorization/roleAssignments/write` permission. The Contributor role explicitly blocks this action; the RBAC Administrator role provides it.

> [!NOTE]
> Use subscription scope if creating a new resource group (`should_create_resource_group = true`). Use resource group scope if the resource group already exists.

**Alternative**: Owner role (grants more permissions than required).

## üöÄ Quick Start

```bash
cd deploy/001-iac
source ../000-prerequisites/az-sub-init.sh
cp terraform.tfvars.example terraform.tfvars
terraform init && terraform apply -var-file=terraform.tfvars
```

> [!IMPORTANT]
> The default configuration creates a **private AKS cluster** (`should_enable_private_aks_cluster = true`). After deploying infrastructure, you must deploy the [VPN Gateway](vpn/) and connect before running `kubectl` commands or [002-setup](../002-setup/) scripts.

## ‚öôÔ∏è Configuration

### Core Variables

| Variable          | Description                              | Required            |
|-------------------|------------------------------------------|---------------------|
| `environment`     | Deployment environment (dev, test, prod) | Yes                 |
| `resource_prefix` | Resource naming prefix                   | Yes                 |
| `location`        | Azure region                             | Yes                 |
| `instance`        | Instance identifier                      | No (default: "001") |
| `tags`            | Resource group tags                      | No (default: {})    |

### AKS System Node Pool

| Variable                               | Description                              | Default            |
|----------------------------------------|------------------------------------------|--------------------|
| `system_node_pool_vm_size`             | VM size for AKS system node pool         | `Standard_D8ds_v5` |
| `system_node_pool_node_count`          | Number of nodes for AKS system node pool | `1`                |
| `system_node_pool_zones`               | Availability zones for system node pool  | `null`             |
| `system_node_pool_enable_auto_scaling` | Enable auto-scaling for system node pool | `false`            |
| `system_node_pool_min_count`           | Minimum nodes when auto-scaling enabled  | `null`             |
| `system_node_pool_max_count`           | Maximum nodes when auto-scaling enabled  | `null`             |

### Feature Flags

| Variable                              | Description                                               | Default |
|---------------------------------------|-----------------------------------------------------------|---------|
| `should_enable_nat_gateway`           | Deploy NAT Gateway for outbound connectivity              | `true`  |
| `should_enable_private_endpoint`      | Deploy private endpoints and DNS zones for Azure services | `true`  |
| `should_enable_private_aks_cluster`   | Make AKS API endpoint private (requires VPN for kubectl)  | `true`  |
| `should_enable_public_network_access` | Allow public access to resources                          | `true`  |
| `should_deploy_postgresql`            | Deploy PostgreSQL Flexible Server for OSMO                | `true`  |
| `should_deploy_redis`                 | Deploy Azure Managed Redis for OSMO                       | `true`  |

### Network Configuration Modes

Three deployment modes are supported based on security requirements:

#### Full Private (Default)

All Azure services use private endpoints and AKS has a private control plane. Requires VPN for all access.

```hcl
# terraform.tfvars (default values)
should_enable_private_endpoint    = true
should_enable_private_aks_cluster = true
```

Deploy VPN Gateway after infrastructure: `cd vpn && terraform apply`

#### Hybrid: Private Services, Public AKS

Azure services (Storage, Key Vault, ACR, PostgreSQL, Redis) use private endpoints, but AKS control plane is publicly accessible. No VPN required for `kubectl` access.

```hcl
# terraform.tfvars
should_enable_private_endpoint    = true
should_enable_private_aks_cluster = false
```

This mode provides security for Azure resources while allowing cluster management without VPN.

#### Full Public

All endpoints are publicly accessible. Not recommended for production without additional hardening.

```hcl
# terraform.tfvars
should_enable_private_endpoint    = false
should_enable_private_aks_cluster = false
```

> [!WARNING]
> Public endpoints expose services to the internet. When using this configuration, you **must** secure cluster workloads:
>
> **AzureML Extension**: Configure HTTPS and restrict access via inference router settings. See [Secure online endpoints](https://learn.microsoft.com/azure/machine-learning/how-to-secure-kubernetes-online-endpoint) and [Inference routing](https://learn.microsoft.com/azure/machine-learning/how-to-kubernetes-inference-routing-azureml-fe).
>
> **OSMO UI**: Enable Keycloak authentication to protect the web interface. See [OSMO Keycloak configuration](https://nvidia.github.io/OSMO/main/deployment_guide/getting_started/deploy_service.html#step-2-configure-keycloak).

### OSMO Workload Identity

Enable managed identity for OSMO services (recommended for production):

```hcl
osmo_config = {
  should_enable_identity   = true
  should_federate_identity = true
  control_plane_namespace  = "osmo-control-plane"
  operator_namespace       = "osmo-operator"
  workflows_namespace      = "osmo-workflows"
}
```

See [variables.tf](variables.tf) for all configuration options.

## üèóÔ∏è Architecture

### Directory Structure

```text
001-iac/
‚îú‚îÄ‚îÄ main.tf                            # Module composition
‚îú‚îÄ‚îÄ variables.tf                       # Input variables
‚îú‚îÄ‚îÄ outputs.tf                         # Output values
‚îú‚îÄ‚îÄ versions.tf                        # Provider versions
‚îú‚îÄ‚îÄ terraform.tfvars                   # Configuration (gitignored)
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ platform/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ networking.tf              # VNet, subnets, NAT Gateway, DNS resolver
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.tf                # Key Vault, managed identities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ observability.tf           # LAW, Monitor, Grafana, AMPLS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage.tf                 # Storage Account
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acr.tf                     # Container Registry
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ azureml.tf                 # ML Workspace
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresql.tf              # PostgreSQL Flexible Server
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis.tf                   # Azure Managed Redis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ private-dns-zones.tf       # Private DNS zones
‚îÇ   ‚îú‚îÄ‚îÄ sil/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aks.tf                     # AKS cluster, node pools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ networking.tf              # AKS subnets, NAT associations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ observability.tf           # Container Insights, Prometheus DCRs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ osmo-federated-credentials.tf  # OSMO workload identity
‚îÇ   ‚îú‚îÄ‚îÄ vpn/                           # VPN Gateway module
‚îÇ   ‚îî‚îÄ‚îÄ automation/                    # Automation Account module
‚îú‚îÄ‚îÄ vpn/                               # Standalone VPN deployment
‚îú‚îÄ‚îÄ dns/                               # OSMO UI DNS configuration
‚îî‚îÄ‚îÄ automation/                        # Scheduled startup deployment
```

### Module Structure

```text
Root Module (001-iac/)
‚îú‚îÄ‚îÄ Platform Module         # Shared Azure services
‚îÇ   ‚îú‚îÄ‚îÄ Networking          # VNet, subnets, NAT Gateway, DNS resolver
‚îÇ   ‚îú‚îÄ‚îÄ Security            # Key Vault (RBAC), managed identities
‚îÇ   ‚îú‚îÄ‚îÄ Observability       # Log Analytics, Monitor, Grafana, AMPLS
‚îÇ   ‚îú‚îÄ‚îÄ Storage             # Storage Account, ACR
‚îÇ   ‚îú‚îÄ‚îÄ Machine Learning    # AzureML Workspace
‚îÇ   ‚îî‚îÄ‚îÄ OSMO Backend        # PostgreSQL, Redis
‚îÇ
‚îî‚îÄ‚îÄ SiL Module              # AKS-specific infrastructure
    ‚îú‚îÄ‚îÄ AKS Cluster         # Azure CNI Overlay, workload identity
    ‚îú‚îÄ‚îÄ GPU Node Pools      # Configurable via node_pools variable
    ‚îî‚îÄ‚îÄ Observability       # Container Insights, Prometheus DCRs
```

### Resources by Category

| Category         | Resources                                                                        |
|------------------|----------------------------------------------------------------------------------|
| Networking       | VNet, subnets (main, PE, AKS, GPU pools), NSG, NAT Gateway, DNS Private Resolver |
| Security         | Key Vault (RBAC mode), ML identity, OSMO identity                                |
| Observability    | Log Analytics, App Insights, Monitor Workspace, Grafana, DCE, AMPLS              |
| Storage          | Storage Account (blob/file), Container Registry (Premium)                        |
| Machine Learning | AzureML Workspace                                                                |
| AKS              | Cluster with Azure CNI Overlay, system pool, GPU node pools                      |
| Private DNS      | 11 core zones (Key Vault, Storage, ACR, ML, AKS, Monitor)                        |
| OSMO Services    | PostgreSQL Flexible Server (HA), Azure Managed Redis                             |

### Conditional Resources

| Condition                        | Resources Created                                        |
|----------------------------------|----------------------------------------------------------|
| `should_enable_private_endpoint` | Private endpoints, 11+ DNS zones, DNS resolver, AMPLS    |
| `should_enable_nat_gateway`      | NAT Gateway, Public IP, subnet associations              |
| `should_deploy_postgresql`       | PostgreSQL server, databases, delegated subnet, DNS zone |
| `should_deploy_redis`            | Redis cache, private endpoint (if PE enabled), DNS zone  |

## üì¶ Modules

| Module                        | Purpose                                                         |
|-------------------------------|-----------------------------------------------------------------|
| [platform](modules/platform/) | Networking, storage, Key Vault, ML workspace, PostgreSQL, Redis |
| [sil](modules/sil/)           | AKS cluster with GPU node pools                                 |
| [vpn](modules/vpn/)           | VPN Gateway module (used by vpn/ standalone deployment)         |

## üì§ Outputs

```bash
terraform output

# AKS cluster details
terraform output -json aks_cluster | jq -r '.name'

# OSMO connection details
terraform output postgresql_connection_info
terraform output managed_redis_connection_info

# Key Vault name (for 002-setup scripts)
terraform output key_vault_name

# DNS server IP (for VPN clients)
terraform output dns_server_ip
```

## üîß Optional Components

Standalone deployments extend the base infrastructure.

### VPN Gateway

Point-to-Site VPN for secure remote access to the private AKS cluster and Azure services.

> [!IMPORTANT]
> **Required for default configuration.** With `should_enable_private_aks_cluster = true`, you cannot run `kubectl` commands or 002-setup scripts without VPN connectivity. To skip VPN, set `should_enable_private_aks_cluster = false` in your `terraform.tfvars`.

```bash
cd vpn
cp terraform.tfvars.example terraform.tfvars
terraform init && terraform apply -var-file=terraform.tfvars
```

See [vpn/README.md](vpn/README.md) for configuration options and [VPN client setup](vpn/README.md#-vpn-client-setup) for connecting from your local machine.

### Private DNS for OSMO UI

Configure DNS resolution for the OSMO UI LoadBalancer after setup from `deploy/002-setup/03-deploy-osmo-control-plane.sh` (requires VPN):

```bash
cd dns
terraform init
terraform apply -var="osmo_loadbalancer_ip=10.0.x.x"
```

See [dns/README.md](dns/README.md) for details.

### Automation Account

Scheduled startup of AKS and PostgreSQL to reduce costs:

```bash
cd automation
cp terraform.tfvars.example terraform.tfvars
terraform init && terraform apply -var-file=terraform.tfvars
```

See [automation/README.md](automation/README.md) for schedule configuration.

## üóëÔ∏è Destroy Infrastructure

Remove Azure resources deployed by Terraform. Clean up cluster components first.

### Cleanup Order

```bash
# 1. OSMO Backend
../002-setup/cleanup/uninstall-osmo-backend.sh

# 2. OSMO Control Plane
../002-setup/cleanup/uninstall-osmo-control-plane.sh

# 3. AzureML Extension
../002-setup/cleanup/uninstall-azureml-extension.sh

# 4. GPU Infrastructure
../002-setup/cleanup/uninstall-robotics-charts.sh

# 5. VPN (if deployed)
cd vpn && terraform destroy -var-file=terraform.tfvars

# 6. Main Infrastructure
terraform destroy -var-file=terraform.tfvars
```

### Terraform Destroy

Preserves state and allows redeployment:

```bash
cd deploy/001-iac
terraform plan -destroy -var-file=terraform.tfvars
terraform destroy -var-file=terraform.tfvars
```

### Delete Resource Group

Fastest cleanup method (removes all resources regardless of how they were created):

```bash
terraform output -raw resource_group | jq -r '.name'
az group delete --name <resource-group-name> --yes --no-wait
```

## üîç Troubleshooting

Issues and resolutions encountered during infrastructure deployment and destroy.

### Destroy Takes a Long Time

Terraform destroy removes resources in dependency order. Private Endpoints, AKS clusters, and PostgreSQL servers commonly take 5-10 minutes each. Full destruction typically takes 20-30 minutes.

Monitor remaining resources during destruction:

```bash
az resource list --resource-group <resource-group> --query "[].{name:name, type:type}" -o table
```

### Soft-Deleted Resources Block Redeployment

Azure retains certain deleted resources in a soft-deleted state. Redeployment fails when Terraform attempts to create a resource with the same name as a soft-deleted one.

| Resource           | Soft Delete      | Retention Period         | Blocks Redeployment             |
|--------------------|------------------|--------------------------|---------------------------------|
| Key Vault          | Mandatory        | 7-90 days (configurable) | Yes                             |
| Azure ML Workspace | Mandatory        | 14 days (fixed)          | Yes                             |
| Container Registry | Opt-in (preview) | 1-90 days (configurable) | No (disabled by default)        |
| Storage Account    | Recovery only    | 14 days                  | No (same-name creation allowed) |

#### Purge Soft-Deleted Key Vault

List soft-deleted vaults and purge:

```bash
# List soft-deleted Key Vaults
az keyvault list-deleted --subscription <subscription-id> --resource-type vault -o table

# Purge a specific vault
az keyvault purge --subscription <subscription-id> --name <key-vault-name>
```

> [!NOTE]
> Key Vaults with `purge_protection_enabled = true` cannot be purged and must wait for retention expiry. This configuration defaults to `should_enable_purge_protection = false`.

#### Purge Soft-Deleted Azure ML Workspace

Azure ML workspaces enter soft-delete for 14 days after deletion. List via Azure Portal under **Azure Machine Learning > Manage deleted workspaces**.

Permanently delete via CLI:

```bash
az ml workspace delete \
  --name <workspace-name> \
  --resource-group <resource-group> \
  --permanently-delete
```

### Terraform State Mismatch

Resources manually deleted or created outside Terraform cause state mismatches.

#### Refresh State for Deleted Resources

Resources deleted outside Terraform leave orphaned state entries:

```bash
terraform refresh -var-file=terraform.tfvars
terraform plan -var-file=terraform.tfvars
```

#### Import Existing Resources

Resources created outside Terraform can be imported into state:

```bash
# Identify the resource address from terraform plan output
terraform plan -var-file=terraform.tfvars

# Import using resource address and Azure resource ID
terraform import -var-file=terraform.tfvars '<resource_address>' '<azure_resource_id>'

# Example: Import a resource group
terraform import -var-file=terraform.tfvars 'module.platform.azurerm_resource_group.main' '/subscriptions/<sub-id>/resourceGroups/<rg-name>'

# Example: Import an AKS cluster
terraform import -var-file=terraform.tfvars 'module.sil.azurerm_kubernetes_cluster.main' '/subscriptions/<sub-id>/resourceGroups/<rg-name>/providers/Microsoft.ContainerService/managedClusters/<aks-name>'
```

After import, run `terraform plan` to verify the imported resource matches the configuration.

### Resource Locks Prevent Deletion

Management locks block deletion operations:

```bash
# List locks on resource group
az lock list --resource-group <resource-group> -o table

# Delete a specific lock
az lock delete --name <lock-name> --resource-group <resource-group>
```
